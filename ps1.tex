\documentclass{article}
\usepackage{amsmath}
\newcommand{\matr}[1]{\mathbf{#1}}

\begin{document}
\pagenumbering{arabic}
\section{Simple? Matrix Problems}
	\subsection{} % 1.a
	This would only work in a column vector with a same size. Let's say it's a size-N vector.

	\begin{equation*}
	vec(\matr{a} \times \matr{b^{T}}) = b \otimes a
	\end{equation*}

	$ \matr{a} \times \matr{b^{T}}$ would have 

	\subsection{} % 1.b

	\begin{equation*}
	vec(\matr{A})^{T} * vec(\matr{B}) = \matr{tr(\matr{A^{T}}*\matr{B})} 
	\end{equation*}	

	\subsection{} % 1.c
	Since positive semidefinite matrices have non-negative diagnoal entries, which our matrix does have, $X^{T}X$ 
	is positive semidefinite.

\section {Problem 2}
	$P(B) = 0.7\%$\\
	$P(M \mid B) = 90\%$\\
	$P(M \mid \neg B) = 8\%$\\
	$P(B \mid M)  = \frac{P(M \mid B) P(B)}{P(M)} = \frac{90\% * 0.7\%}{98\%} = \mathbf{0.6\%}$

\section {Problem 3}
	\subsection{Matrix Version} % 3.1
		\subsubsection{} % 3.1.a

		\subsubsection{} % 3.1.b

	\subsection{4D Tensor Version} % 3.2
		\subsubsection{} % 3.2.a
		We start with a 4D tensor with dimension of M X N 3 X K. Let's name this img. 
		We'll begin our series of operations with applying vec() on our tensor.\\

		\begin{equation*}
		\matr{A} = vec(img)
		\end{equation*}

		Now vec(img) is going to return a A, a matrix of size (M X N X 3) by K. We are going to multiply this with 3 by 1 column vector of 1s. 
		This would return sum of all images at their respective pixel and channel.

		\begin{equation*}
		\matr{V_{RGB}} = \matr{A} * 
						\begin{bmatrix} 
						1 \\ 1 \\ 1  
						\end{bmatrix} 
		\end{equation*}	
		
		So now we have $\matr{V_{RGB}}$ which is a sum of all images at their respectivel pixel and channel. We need to reshape this so that
		we have (M X N) by 3 matrix. This matrix would have M x N image but color-sorted in each column.  
		We can then multiply by a column vector of 1s similar to what we did before to sum up each pixel over all color channels. \\
		\\
		We'll do this by using vec-transpose and then multiplying by a column of 1s with size of 3 by 1

		\begin{equation*}
		\matr{V_{Greyscale}} = \matr{V_{RGB}} * 
						\begin{bmatrix} 
						1 \\ 1 \\ 1  
						\end{bmatrix} 
		\end{equation*}	

		Now we have a (M X N) column vector with each pixel summed over all images and color. We need to divide this by 3 * K to get the mean
		image.

		\begin{equation*}
		\matr{V_{\mu}} = \matr{V_{Greyscale}} / (3*K) 
		\end{equation*}	

		We are done.

		\subsubsection{} % 3.2.b
	
		We start once again with applying vec() on our tensor.\\

		\begin{equation*}
		\matr{A} = vec(img)
		\end{equation*}
		
		But we'll transpose the matrix, so all the red channel would be on one column. Blue on other and so on. 
		
\section{This problem was pop}
	We are given a vector X and we are supposed to come up with a matrix that when multiplied that returns a vec-operated coefficient matrix.\\
	Here are three things that we have to do:\\
	\\
	1. We have to do calculate coefficients of 1024 samples\\
	2. We have to hop 512 samples\\
	3. We need to apply a Hann Window\\
	\\
	Let's do 1 and 2 at the same time and then do 3.

	\subsection{Calculate and Hop}
		We have a column vector of sample and to accomplish 1 and 2, we are gonna have a matrix that's gonna look something like this.
		
		\begin{equation*}
		\begin{bmatrix} 
		e^{\frac{-2 \pi j*0*0}{1024}} e^{\frac{-2 \pi j*0*1}{1024}} ... \ e^{\frac{-2 \pi j*0*1023}{1024}} 0 \ 0 \ .. \ 0 \\
		e^{\frac{-2 \pi j*1*0}{1024}} e^{\frac{-2 \pi j*1*1}{1024}} ... \ e^{\frac{-2 \pi j*1*1023}{1024}} 0 \ 0 \ .. \ 0 \\
		e^{\frac{-2 \pi j*1023*0}{1024}} e^{\frac{-2 \pi j*1023*1}{1024}} ... \ e^{\frac{-2 \pi j*1023*1023}{1024}} 0 \ 0 \ .. \ 0 \\
		...\\
		0_{0} \ 0_{1} \ .. 0_{511} \ e^{\frac{-2 \pi j*0*0}{1024}} e^{\frac{-2 \pi j*0*1}{1024}} ... \ e^{\frac{-2 \pi j*0*1023}{1024}} \ 0 \ 0 \ .. \ 0 \\
		0_{0} \ 0_{1} \ .. 0_{511} \ e^{\frac{-2 \pi j*1*0}{1024}} e^{\frac{-2 \pi j*1*1}{1024}} ... \ e^{\frac{-2 \pi j*1*1023}{1024}} \ 0 \ 0 \ .. \ 0 \\
		0_{0} \ 0_{1} \ .. 0_{511} \ e^{\frac{-2 \pi j*1023*0}{1024}} e^{\frac{-2 \pi j*1023*1}{1024}} ... \ e^{\frac{-2 \pi j*1023*1023}{1024}} \ 0 \ 0 \ .. \ 0 \\
		...\\
		0 \ 0 \ .. \ 0 \ e^{\frac{-2 \pi j*0*0}{1024}} e^{\frac{-2 \pi j*0*1}{1024}} ... \ e^{\frac{-2 \pi j*0*1023}{1024}}\\
		0 \ 0 \ .. \ 0 \ e^{\frac{-2 \pi j*1*0}{1024}} e^{\frac{-2 \pi j*1*1}{1024}} ... \ e^{\frac{-2 \pi j*1*1023}{1024}}\\
		0 \ 0 \ .. \ 0 \ e^{\frac{-2 \pi j*1023*0}{1024}} e^{\frac{-2 \pi j*1023*1}{1024}} ... \ e^{\frac{-2 \pi j*1023*1023}{1024}}\\
		\end{bmatrix}
		\end{equation*}	
		

	
				
		\end{document}